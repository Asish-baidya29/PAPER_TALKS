# ğŸ“„ PaperTalks

**PaperTalks** is a lightweight, locally run application that allows users to interact with any PDF or text document. It supports two core functionalities:
1. Generate **Multiple Choice Questions (MCQs)** using classical NLP.
2. Ask **context-aware questions** to the document using a custom **Retrieval-Augmented Generation (RAG)** pipeline.

---

## ğŸš€ Features

- ğŸ“š Upload any PDF or `.txt` file.
- ğŸ§  Generate MCQs using spaCy (no LLMs involved).
- ğŸ’¬ Chat with the document using LangChain-based RAG pipeline.
- ğŸ§¾ Runs fully locally â€” uses **FAISS** for retrieval and **TinyLlama** for generation.
- ğŸ–¥ï¸ Clean, interactive UI built with **Streamlit**.

---

## ğŸ› ï¸ Tech Stack

- **Python**
- **LangChain** â€“ for building modular retrieval and generation chains.
- **FAISS** â€“ for fast vector similarity search.
- **TinyLlama (1.1B)** â€“ as a local LLM for context-based generation.
- **spaCy** â€“ for traditional NLP tasks and MCQ generation.
- **Streamlit** â€“ for the front-end interface.

---

## âš™ï¸ How It Works

### ğŸ§  MCQ Generator
- Utilizes spaCy to perform named entity recognition (NER) and pattern matching.
- Applies rule-based logic to formulate question-answer pairs.
- Designed for speed and runs without any LLM or external API.

### ğŸ’¬ RAG-based Document Chat
- Documents are chunked and embedded using LangChainâ€™s document loaders and embedding models.
- Vector representations are stored in a local FAISS index.
- On user query:
  - Top-k relevant chunks are retrieved using cosine similarity.
  - A prompt is dynamically generated using the query and retrieved context.
  - **TinyLlama**, running locally, generates the final response.
- Includes fallback handling for low-relevance cases.

---

## ğŸ§ª Running Locally

